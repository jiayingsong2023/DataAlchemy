#!/usr/bin/env python3
"""
é¢„ä¸‹è½½è®­ç»ƒæ‰€éœ€çš„åŸºç¡€æ¨¡å‹
åœ¨è®­ç»ƒå‰è¿è¡Œæ­¤è„šæœ¬å¯ä»¥é¢„å…ˆä¸‹è½½æ¨¡å‹ï¼Œé¿å…è®­ç»ƒæ—¶ç­‰å¾…ä¸‹è½½
"""
import sys
import os
import types
from importlib.machinery import ModuleSpec

# Fix for ROCm's PyTorch 2.9.1 circular import bug in torch.distributed.tensor
# Same fix as in train.py

class DummyDTensor:
    pass

class DummyPlacement:
    def __init__(self, *args, **kwargs): pass

class TensorSubmoduleHook:
    def find_spec(self, name, path, target=None):
        if name == 'torch.distributed.tensor' or name.startswith('torch.distributed.tensor.'):
            class Loader:
                def create_module(self_loader, spec):
                    module = types.ModuleType(spec.name)
                    module.__path__ = []
                    if spec.name == 'torch.distributed.tensor':
                        module.DTensor = DummyDTensor
                        module.Shard = DummyPlacement
                        module.Replicate = DummyPlacement
                        module.Partial = DummyPlacement
                    elif '_dtensor_spec' in spec.name:
                        module.DTensorSpec = type('DTensorSpec', (), {})
                        module.TensorMeta = type('TensorMeta', (), {})
                    elif 'placement_types' in spec.name:
                        module.Placement = DummyPlacement
                        module.Shard = DummyPlacement
                        module.Replicate = DummyPlacement
                        module.Partial = DummyPlacement
                        module._StridedShard = DummyPlacement
                    elif 'device_mesh' in spec.name:
                        module._mesh_resources = type('_mesh_resources', (), {})
                        module.DeviceMesh = type('DeviceMesh', (), {})
                    return module
                def exec_module(self_loader, module):
                    pass
            return ModuleSpec(name, Loader())
        return None

if not any(isinstance(hook, TensorSubmoduleHook) for hook in sys.meta_path):
    sys.meta_path.insert(0, TensorSubmoduleHook())

import torch
import torch.distributed

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from config import get_model_config
from transformers import AutoTokenizer, AutoModelForCausalLM

def preload_model():
    """é¢„ä¸‹è½½æ¨¡å‹å’Œtokenizer"""
    print("=" * 60)
    print("é¢„ä¸‹è½½ LoRA è®­ç»ƒåŸºç¡€æ¨¡å‹")
    print("=" * 60)
    
    # è·å–æ¨¡å‹é…ç½®
    model_c = get_model_config("model_c")
    model_id = model_c.get("model_id", "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T")
    
    print(f"\næ¨¡å‹ID: {model_id}")
    print(f"ç¼“å­˜ç›®å½•: ~/.cache/huggingface/hub/\n")
    
    # 1. ä¸‹è½½ Tokenizer
    print("ğŸ“¥ æ­£åœ¨ä¸‹è½½ Tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        print(f"âœ… Tokenizer ä¸‹è½½å®Œæˆ")
        print(f"   - Vocab size: {len(tokenizer)}")
    except Exception as e:
        print(f"âŒ Tokenizer ä¸‹è½½å¤±è´¥: {e}")
        return False
    
    # 2. ä¸‹è½½ Model
    print("\nğŸ“¥ æ­£åœ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            dtype=torch.float16,
            device_map="cpu",
            low_cpu_mem_usage=True,
        )
        print(f"âœ… æ¨¡å‹æ–‡ä»¶ä¸‹è½½å®Œæˆ")
        print(f"   - æ¨¡å‹ç±»å‹: {type(model).__name__}")
        print(f"   - å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        del model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        import gc
        gc.collect()
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        return False
    
    print("\n" + "=" * 60)
    print("âœ… é¢„ä¸‹è½½å®Œæˆï¼æ¨¡å‹å·²ç¼“å­˜åˆ°æœ¬åœ°")
    print("   ç°åœ¨å¯ä»¥è¿è¡Œ 'uv run train-lora' è¿›è¡Œè®­ç»ƒ")
    print("=" * 60)
    return True

if __name__ == "__main__":
    try:
        success = preload_model()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
