# Data Alchemy: Enterprise RAG + LoRA Multi-Agent System

This project is an enterprise-grade AI system that combines **Data Cleaning**, **Multi-Agent Coordination**, **LoRA Fine-tuning**, and **RAG (Retrieval-Augmented Generation)**. Optimized for AMD GPUs on Windows (ROCm), it transforms enterprise internal data (Jira, Git, Docs) into a reliable knowledge assistant.

## ğŸš€ Key Features

-   **Multi-Agent Architecture**:
    -   **Agent A (Cleaner)**: Hybrid cleaning (WSL/Spark + Windows/LLM).
    -   **Agent B (Trainer)**: Specialized LoRA domain training.
    -   **Agent C (Knowledge)**: FAISS-powered high-speed vector search.
    -   **Agent D (Finalist)**: Intelligent fusion of RAG facts and LoRA intuition.
    -   **Agent S (Scheduler)**: Automates periodic ingestion and training.
-   **Cross-Environment ETL**: Uses Spark in WSL for rough cleaning and LLMs in Windows for refinement, solving dependency conflicts.
-   **ROCm Optimized**: Tailored for AMD Radeonâ„¢ GPUs using specific ROCm for Windows wheels.

---

## ğŸ› ï¸ Getting Started

### 1. Prerequisites
-   **AMD GPU**: Compatible with ROCm.
-   **WSL2**: Installed on Windows.
-   **uv**: [Install uv](https://github.com/astral-sh/uv).

### 2. Environment Setup

**Main Project (Windows - AI & Refinement):**
```powershell
uv sync
```

**Spark Worker (WSL - Data Cleaning):**
```bash
# In WSL
cd /mnt/c/Users/<user>/<project path>/spark_etl_standalone
for example, cd /mnt/c/Users/Administrator/work/lora/spark_etl_standalone
uv sync
```

### 3. Running the Pipeline

The system supports two cleaning modes:
-   **`spark` mode (Recommended)**: Uses Spark in WSL for heavy data cleaning and chunking. Ideal for large datasets.
-   **`python` mode**: Pure Python cleaning on Windows. Zero setup required, ideal for small datasets or quick testing.

#### Step 1: Ingestion (Agent A + Agent C)
Rough cleaning (Spark/Python) -> Refinement (LLM) -> Indexing (FAISS).

**1. Rough Cleaning only (Washing):**
```powershell
# Using Spark (WSL) - Recommended for scale
uv run data-alchemy ingest --mode spark --stage wash

# OR Using Pure Python (Windows) - No WSL required
uv run data-alchemy ingest --mode python --stage wash
```
-   Produces `data/cleaned_corpus.jsonl` (for SFT) and `data/rag_chunks.jsonl` (for RAG).

**2. Refinement & Indexing only:**
```powershell
# Convert rough data to SFT pairs and build knowledge index
uv run data-alchemy ingest --stage refine --synthesis --max_samples 50
```
-   Expects `cleaned_corpus.jsonl` and `rag_chunks.jsonl` to exist.

**3. Full Ingestion Pipeline (Default):**
```powershell
# Rough cleaning + LLM Synthesis + FAISS Indexing in one go
uv run data-alchemy ingest --mode spark --synthesis --max_samples 50
```
-   **Rough Cleaning**: `Agent A` produces `data/cleaned_corpus.jsonl`.
-   **Refinement**: `SFT Generator` converts rough data into `data/sft_train.jsonl`.
-   **Indexing**: `Agent C` builds FAISS index from `data/rag_chunks.jsonl`.

#### Step 2: Training (Agent B)
Fine-tune the model using the refined SFT data.
```powershell
uv run train-lora
```

#### Step 3: Interactive Chat
Combine RAG facts and LoRA intuition for expert answers.
```powershell
uv run chat
```

#### Step 4: Auto-Evolution
You can run the full cycle (Wash -> Refine -> Index -> Train) either once or periodically.

**1. One-shot Full Cycle:**
```powershell
# Run the entire pipeline once and exit
uv run schedule-sync full-cycle --mode spark --synthesis
```

**2. Periodic Schedule (Agent S):**
```powershell
# Auto-evolve every 24 hours (Scheduler will stay active)
uv run schedule-sync schedule --mode spark --interval 24 --synthesis
```

---

## ğŸ—ï¸ Project Structure

```
.
â”œâ”€â”€ src/                        # Main AI Stack (Windows)
â”‚   â”œâ”€â”€ agents/                 # Specialized Agents (A, B, C, D, S)
â”‚   â”œâ”€â”€ rag/                    # Vector Database logic
â”‚   â”œâ”€â”€ etl/                    # Python ETL & SFT Refinement
â”‚   â”œâ”€â”€ config.py               # Path & API configuration
â”‚   â””â”€â”€ run_agents.py           # Unified entry point
â”œâ”€â”€ spark_etl_standalone/       # Spark Worker (WSL)
â”‚   â”œâ”€â”€ main.py                 # Spark ETL Entry point
â”‚   â””â”€â”€ pyproject.toml          # Lightweight Spark dependencies
â”œâ”€â”€ data/                       # Shared Data Storage
â”‚   â”œâ”€â”€ raw/                    # Input: Git, Jira, Docs
â”‚   â”œâ”€â”€ cleaned_corpus.jsonl    # Stage 1: Rough cleaned (Spark)
â”‚   â”œâ”€â”€ sft_train.jsonl         # Stage 2: Refined (LLM)
â”‚   â””â”€â”€ faiss_index.bin         # Knowledge Index
â”œâ”€â”€ docs/                       # Technical Documentation
â”‚   â””â”€â”€ ARCHITECTURE.md         # Detailed system design
â”œâ”€â”€ .env                        # API Keys (DEEPSEEK_API_KEY)
â””â”€â”€ pyproject.toml              # Main project config
```

## ğŸ”§ Troubleshooting

-   **WSL Connection**: Ensure WSL can access `/mnt/c/`.
-   **API Keys**: Ensure `DEEPSEEK_API_KEY` is set in `.env`.
-   **ROCm Hangs**: The system uses `os._exit(0)` to prevent ROCm-related hangs on Windows termination.
