# Multi-Agent LoRA + RAG Knowledge Hub (AMD ROCm)

This project is an enterprise-grade AI system that combines **Multi-Agent Coordination**, **LoRA Fine-tuning**, and **RAG (Retrieval-Augmented Generation)**. Optimized for AMD GPUs on Windows (ROCm), it transforms internal data (Jira, Git, Docs) into a reliable knowledge assistant.

## ğŸš€ Key Features

- **Multi-Agent Architecture**: 
    - **Agent A (Cleaner)**: Dual-track cleaning for SFT and RAG.
    - **Agent B (Trainer)**: Specialized LoRA domain training.
    - **Agent C (Knowledge)**: FAISS-powered high-speed vector search.
    - **Agent D (Finalist)**: Intelligent fusion of RAG facts and LoRA intuition.
    - **Agent S (Scheduler)**: Automates periodic data ingestion and training.
- **RAG + LoRA Fusion**: Uses a hybrid approach where RAG provides the "facts" and LoRA provides the "domain understanding".
- **FAISS Vector DB**: Locally managed, persistent vector storage.
- **ROCm Optimized**: Tailored for AMD Radeonâ„¢ 8060S / AI Max+ 395.

## ğŸ› ï¸ Prerequisites

- **AMD GPU**: Compatible with ROCm (e.g., Radeon 7000/8000 series).
- **uv**: [Install uv](https://github.com/astral-sh/uv).
- **FAISS**: Installed via `uv sync`.
- **API Key**: Required for Synthesis and Agent D.

## âš™ï¸ Configuration

1. **Create .env file**: Copy `.env.example` to `.env` or create it manually in the project root.
   ```env
   DEEPSEEK_API_KEY=your_actual_key_here
   ```
2. **Security**: The `.env` file is ignored by Git to prevent leaking your keys.

## ğŸš¦ Getting Started

### 1. Environment Setup
```powershell
uv sync
```

### 2. Running the Agentic Pipeline
The system is controlled via a unified entry point. You can use the convenience commands defined in `pyproject.toml`:

#### Step 1: Ingestion (Agent A + Agent C)
Clean raw data, synthesize knowledge via LLM (optional), and build the FAISS vector index.
```powershell
# Basic ingestion (cleaning + indexing)
uv run data-alchemy ingest

# Ingestion with LLM Synthesis (generate SFT data)
uv run data-alchemy ingest --synthesis --max_samples 10
```

#### Step 2: Training (Agent B)
Perform LoRA fine-tuning on the cleaned corpus.
```powershell
uv run train-lora
```

#### Step 3: Interactive Chat (Agent B + C + D)
Start the multi-agent chat interface.
```powershell
uv run chat
```

#### Step 4: Auto-Evolution (Agent S)
Enable the scheduler to automatically run ingest and train periodically.
```powershell
# Auto-evolve every 24 hours
uv run schedule-sync schedule --interval 24

# Auto-evolve with LLM synthesis enabled
uv run schedule-sync schedule --interval 24 --synthesis
```

## ğŸ—ï¸ Project Structure

```
.
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ agents/             # Multi-Agent Implementations
â”‚   â”‚   â”œâ”€â”€ coordinator.py  # Task Orchestrator
â”‚   â”‚   â”œâ”€â”€ agent_a.py      # Data cleaning logic
â”‚   â”‚   â”œâ”€â”€ agent_b.py      # Model intuition & training
â”‚   â”‚   â”œâ”€â”€ agent_c.py      # Vector search & Rerank
â”‚   â”‚   â””â”€â”€ agent_d.py      # Result fusion via DeepSeek
â”‚   â”œâ”€â”€ rag/                # Vector Database Core
â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”‚   â””â”€â”€ retriever.py
â”‚   â”œâ”€â”€ spark_etl/          # ETL Engines
â”‚   â”œâ”€â”€ run_agents.py       # Unified Entry Point logic
â”‚   â”œâ”€â”€ train.py            # LoRA Training script
â”‚   â””â”€â”€ inference.py        # Chat interface script
â”œâ”€â”€ docs/                   # Documentation & Research
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ Data_Alchemy.txt
â”‚   â””â”€â”€ implementation_plan.md
â”œâ”€â”€ scripts/                # Utility & Test scripts
â”‚   â”œâ”€â”€ test_gpu.py
â”‚   â””â”€â”€ check_torch.py
â”œâ”€â”€ data/                   # Data Storage (Local)
â”‚   â”œâ”€â”€ raw/                # Input data
â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”œâ”€â”€ rag_chunks.jsonl
â”‚   â””â”€â”€ faiss_index.bin
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

## ğŸ§  How Fusion Works
When you ask a question:
1. **Agent C** retrieves the most relevant documentation chunks from FAISS.
2. **Agent B** generates a preliminary answer based on its fine-tuned weights (LoRA Intuition).
3. **Agent D** receives the user query, the RAG evidence, and the LoRA intuition.
4. **DeepSeek** performs the final synthesis, prioritizing facts from RAG while using LoRA's domain understanding.

## ğŸ”§ Troubleshooting
- **Conflict in Dependencies**: The project requires `python == 3.12`. `uv sync` will handle this automatically.
- **Index Not Found**: Ensure you run `ingest` before `chat`.
- **API Errors**: Ensure your `DEEPSEEK_API_KEY` is correctly set in the `.env` file.
