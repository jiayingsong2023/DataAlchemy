# DataAlchemy RoadMap

## \-从数据炼金术到数字孪生

# 短期规划

1. ## 数据源拓展

   选择spark作为数据粗洗工具是看重了其丰富的生态系统，下一步的工作主要是将数据输入从文件系统改为S3。以便更多云原生应用可以接入ETL。

   其次用基于Linux的spark集群取代wsl standalone版本的spark

2. ## LLM模型可插拔结构

   本项目涉及4个LLM，模型A用于将粗洗数据转换成SFT需要的精炼数据，模型B用于做分词和嵌入，模型C用于SFT（LoRA）的基座模型，模型D用于最后回答问题的决策。  
     
   模型C是项目的关键，当长期基于企业数据或者个人数据的训练很可能使得模型的智能退化，在LLM尚不能自我进化之前，不定期更新基座模型是个不错的解决方案。更新基座模型以为着之前的微调就白费了，所以保留并提纯精炼数据成为必要。企业或个人数据，包括用户在使用本项目过程中产生的数据，将成为进一步优化模型C的重要数据资产。  
     
   模型D也是应该不断升级的，这样才能跟上AI发展的步伐。模型A和B完全是根据用户的喜好和成本考虑来做更换了。

3. ## 推理优化

   现有的LLM，不管是ChatGPT、Gemini、Grok、Claude还是DeepSeek、Qwen，它们的能力可能远比我们想象的强，从RAG、Prompt Engineering，ICL到COT其实都是在激发LLM的潜能。一个AI系统是不是好用关键是能不能用好它。  
     
   首先是让他快起来，对于个人用户需要引入Ollama框架，对于企业用户适配vLLM.其次让他聪明起来，在模型还不能进化之前引入多角色智能体或许是个正确的决定。  
   

4. ## webUI
   现有版本的webUI是一个个人预览版，将来应该支持更多功能，比如多用户登录，用户数据隔离，数据清洗及训练参数的配置...
   

5. ## CICD pipeline

   虽然把其列入短期计划，因为本人额度优先，短期不会上线CICD pipeline.  
   

# 长期规划

1. ## 可进化的LLM

   

2. ## 数字孪生